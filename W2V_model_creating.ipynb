{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Word2Vec model creation\n",
    "This script scrapes anime reviews from MyAnimeList Forum, preprocesses the text data, and trains a Word2Vec model on the cleaned text"
   ],
   "id": "dd25f886bada0e7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Download nltk data if not already installed\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function returns: data (list of str) - a list of concatenated anime titles and descriptions from the reviews\n",
    "def load_data_from_myanimelist(base_url, max_pages):\n",
    "    data = []\n",
    "    response = requests.get(base_url)\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        print(\"Getting data from myAnimeList...\")\n",
    "        for i in range(2, max_pages+1):\n",
    "            url = f\"{base_url}&p={i}\"\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            # Extract review descriptions\n",
    "            descriptions = [element.text.strip() for element in soup.find_all('div', {'class': 'text'})] \n",
    "            # Extract anime titles\n",
    "            anime_titles = [element.text.strip() for element in soup.find_all('a', {'class': 'title ga-click'})]\n",
    "            # Combine titles and descriptions\n",
    "            data += [f\"{title}. {description}\" for title, description in zip(anime_titles, descriptions)]\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "myanimelist_url = 'https://myanimelist.net/reviews.php?t=anime&filter_check=&filter_hide=&preliminary=on&spoiler=on'\n",
    "reviews_data = load_data_from_myanimelist(myanimelist_url, max_pages=3200)\n",
    "\n",
    "# Ensure reviews_data is not None\n",
    "if reviews_data is None:\n",
    "    reviews_data = []\n",
    "\n",
    "# Function returns: tokens (list of str) - the cleaned and tokenized words from the input text \n",
    "def clean_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert to lowercase and remove non-alphabetic tokens\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    # Define and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Tokenize and clean the reviews data\n",
    "tokenized_reviews = [clean_text(review) for review in reviews_data]\n",
    "\n",
    "# Train a Word2Vec model on the tokenized reviews\n",
    "model = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, min_count=1, workers=4)\n",
    "print(\"Model built successfully.\")\n",
    "\n",
    "model.save('./models/anime_word2vec_model')"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Updating Word2Vec model with anime titles\n",
    "This script is designed to update a pre-trained Word2Vec model with anime titles from a MyAnimeList CSV file. This ensures that all anime titles in the list are included in the model, preventing errors during vectorization in other scripts and enriching the vocabulary"
   ],
   "id": "f514bfc269d26100"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:25:37.706247Z",
     "start_time": "2025-01-17T12:25:35.063973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to clean and tokenize text\n",
    "def clean_text(text):\n",
    "    # Check if the input is a string\n",
    "    if isinstance(text, str):\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        return tokens\n",
    "    else:\n",
    "        # Return an empty list if the input is not a string\n",
    "        return []\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "model = Word2Vec.load('./models/anime_word2vec_model')\n",
    "\n",
    "# Load anime data from CSV file\n",
    "df = pd.read_csv('./data/anime_list.csv', header=None, names=['user_score', 'title', 'genres', 'popularity'])\n",
    "\n",
    "# Clean and tokenize anime titles\n",
    "titles = df['title'].apply(clean_text).tolist()\n",
    "\n",
    "# Filter out empty lists\n",
    "titles = [title for title in titles if title]\n",
    "\n",
    "# Update the Word2Vec model with the new data\n",
    "model.build_vocab(titles, update=True)\n",
    "model.train(titles, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "\n",
    "model.save('./models/anime_word2vec_model_updated')"
   ],
   "id": "fb479176deb181cd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dobre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dobre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3b3a7b43cf74713e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
