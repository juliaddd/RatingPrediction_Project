{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data preprocessing\n",
    "This script fetches and processes data from a user's anime list on MyAnimeList"
   ],
   "id": "1f5e3768272cdd87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T15:11:41.034265Z",
     "start_time": "2025-09-23T15:11:40.400818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import configparser\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "USER_NAME = config.get('USER', 'USER_NAME').strip()\n",
    "CLIENT_ID = config.get('USER', 'CLIENT_ID').strip()\n",
    "\n",
    "\n",
    "# Constructing the URL for API request using USER_NAME\n",
    "url = f'https://api.myanimelist.net/v2/users/{USER_NAME}/animelist'\n",
    "# url = f'https://api.myanimelist.net/v2/users/kemmy22/animelist'\n",
    "\n",
    "# Setting headers for the API request, including the CLIENT_ID\n",
    "headers = {\n",
    "    'X-MAL-CLIENT-ID': CLIENT_ID\n",
    "}\n",
    "\n",
    "# Setting parameters for the API to specify limit and fields\n",
    "params = {\n",
    "    'limit': 2000,\n",
    "    'fields': 'list_status{score},alternative_titles{en},mean,genres,popularity',\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Parsing the JSON response into Python dictionary\n",
    "    anime_data = response.json()\n",
    "    \n",
    "    modified_anime_data = []\n",
    "    \n",
    "    # Iterating through each anime entry in the API response to get all the info\n",
    "    for anime in anime_data['data']:\n",
    "        anime_info = anime['node']  # Extracting anime info\n",
    "        list_status = anime.get('list_status', {})  # Extracting list status - anime score\n",
    "        \n",
    "        # Creating a modified anime entry with selected attributes\n",
    "        modified_anime = {\n",
    "            'user_score': list_status.get('score'),\n",
    "            'title': anime_info.get('alternative_titles', {}).get('en'),\n",
    "            'mal_score': anime_info.get('mean'),\n",
    "            'genres': [genre['name'] for genre in anime_info.get('genres', [])],\n",
    "            'popularity': anime_info.get('popularity')\n",
    "        }\n",
    "        \n",
    "        modified_anime_data.append(modified_anime)\n",
    "    \n",
    "    \n",
    "    # Creating a DataFrame\n",
    "    df = pd.DataFrame(modified_anime_data)\n",
    "    print(df.head(2))\n",
    "    \n",
    "    # Saving anime list\n",
    "    csv_file_path = './data/anime_list.csv'\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"Data successfully written to '{csv_file_path}'\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")"
   ],
   "id": "1253b0430cd52ed0",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:25:56.913814Z",
     "start_time": "2025-01-17T12:25:55.890310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data/anime_list.csv')\n",
    "# Loading the pre-trained Word2Vec model\n",
    "word_vectors = Word2Vec.load('./models/anime_word2vec_model_updated')\n",
    "\n",
    "df.columns = df.columns.astype(str)\n",
    "# Handling missing values\n",
    "df['title'] = df['title'].fillna('')\n",
    "df['mal_score'] = df['mal_score'].fillna(df['mal_score'].mean())\n",
    "df['popularity'] = df['popularity'].fillna(df['popularity'].mean())\n",
    "df['user_score'] = df['user_score'].fillna(df['user_score'].mean())\n",
    "df['genres'] = df['genres'].fillna('Unknown')"
   ],
   "id": "5da80c549202925b",
   "execution_count": 121,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:25:56.929458Z",
     "start_time": "2025-01-17T12:25:56.913814Z"
    }
   },
   "cell_type": "code",
   "source": "print(df.isnull().sum())",
   "id": "7a7127f6e1649322",
   "execution_count": 122,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:25:57.023691Z",
     "start_time": "2025-01-17T12:25:56.929458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# One-hot encoding the genres column\n",
    "genres_encoder = OneHotEncoder(sparse_output=False)\n",
    "genres_encoded = genres_encoder.fit_transform(df[['genres']])\n",
    "genres_encoded_df = pd.DataFrame(genres_encoded, columns=genres_encoder.get_feature_names_out(['genres']))\n",
    "df = pd.concat([df, genres_encoded_df], axis=1).drop('genres', axis=1)\n",
    "\n",
    "\n",
    "# Ensuring all column names are strings\n",
    "df.columns = df.columns.astype(str)\n",
    "\n",
    "# Normalizing numerical data using StandardScaler\n",
    "numerical_columns = ['mal_score',  'popularity', 'user_score']\n",
    "scaler = StandardScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "\n",
    "# Defining the target variable Y and features X\n",
    "Y = df['user_score']\n",
    "X = df.drop(['user_score'], axis=1)\n",
    "# X = X.dropna()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ],
   "id": "600143668741d015",
   "execution_count": 123,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:25:57.039736Z",
     "start_time": "2025-01-17T12:25:57.023691Z"
    }
   },
   "cell_type": "code",
   "source": "print(X.isnull().sum())",
   "id": "f53937ad8222fb63",
   "execution_count": 124,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Algorithms",
   "id": "a0c178c79779ea08"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Support Vector Regression\n",
    "\n",
    "Support Vector Machineâ€ (SVM) is a supervised learning machine learning algorithm that can be used for both classification or regression challenges.\n",
    "\n",
    "Unlike SVMs used for classification tasks, SVR Model seeks a hyperplane that best fits the data points in a continuous space. This is achieved by mapping the input variables to a high-dimensional feature space and finding the hyperplane that maximizes the margin (distance) between the hyperplane and the closest data points, while also minimizing the prediction error.\n"
   ],
   "id": "124f58291780f8e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:25:57.577740Z",
     "start_time": "2025-01-17T12:25:57.039736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Create and train an SVM model with RBF kernel\n",
    "svm_model = SVR(kernel='rbf', C=100, gamma=0.1)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svr = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae_svr = mean_absolute_error(y_test, y_pred_svr)\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "print(f\"Mean absolute error: {mae_svr:.2f}\")\n",
    "print(f\"Mean squared error: {mse_svr:.2f}\")\n"
   ],
   "id": "9c16f6591e111647",
   "execution_count": 125,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Linear Regression\n",
    "\n",
    " Linear regression is a model that estimates the linear relationship between a scalar response (dependent variable) and one or more explanatory variables (regressor or independent variable)"
   ],
   "id": "6af6664fc9ce1f77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:25:58.637908Z",
     "start_time": "2025-01-17T12:25:57.577740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean absolute error: {mae:.2f}\")\n",
    "print(f\"Mean squared error: {mse:.2f}\")"
   ],
   "id": "8cea29d4e767f695",
   "execution_count": 126,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Decision Tree\n",
    "\n",
    "A decision tree, which has a hierarchical structure made up of root, branches, internal, and leaf nodes, is a non-parametric supervised learning approach used for classification and regression applications.\n",
    "\n",
    "Starting at the Root -> Asking the Best Questions -> Branching Out -> Repeating the Process"
   ],
   "id": "bbbb7b1b4055a8eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:25:58.732037Z",
     "start_time": "2025-01-17T12:25:58.639425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = regressor.predict(X_test)\n",
    "\n",
    "mae_dt = mean_absolute_error(y_test, y_pred_dt)\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "print(f\"Mean absolute error: {mae_dt:.2f}\")\n",
    "print(f\"Mean squared error: {mse_dt:.2f}\")"
   ],
   "id": "ae6c2ae9b455d4f3",
   "execution_count": 127,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Random Forest\n",
    "\n",
    "Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that works by creating a multitude of decision trees during training. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the output is the average of the predictions of the trees."
   ],
   "id": "aa4048378a4dbffa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:00.767180Z",
     "start_time": "2025-01-17T12:25:58.732037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressor = RandomForestRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = regressor.predict(X_test)\n",
    "\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(f\"Mean absolute error: {mae_rf:.2f}\")\n",
    "print(f\"Mean squared error: {mse_rf:.2f}\")"
   ],
   "id": "31378cbd0f2d0417",
   "execution_count": 128,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## K Nearest Neighbors\n",
    "\n",
    "KNN regression is a non-parametric method that approximates the association between independent variables and the continuous outcome by averaging the observations in the same neighbourhood."
   ],
   "id": "cef3e485804dc060"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:00.845714Z",
     "start_time": "2025-01-17T12:26:00.767180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "regressor = KNeighborsRegressor(n_neighbors=5, weights='distance')\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = regressor.predict(X_test)\n",
    "\n",
    "mae_knn = mean_absolute_error(y_test, y_pred_knn)\n",
    "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
    "print(f\"Mean absolute error: {mae_knn:.2f}\")\n",
    "print(f\"Mean squared error: {mse_knn:.2f}\")"
   ],
   "id": "af4e2def3194ff95",
   "execution_count": 129,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Gradient Boosting is a powerful boosting algorithm that combines several weak learners into strong learners, in which each new model is trained to minimize the loss function such as mean squared error or cross-entropy of the previous model using gradient descent. In each iteration, the algorithm computes the gradient of the loss function with respect to the predictions of the current ensemble and then trains a new weak model to minimize this gradient. The predictions of the new model are then added to the ensemble, and the process is repeated until a stopping criterion is met."
   ],
   "id": "79458c672bbaea04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:01.764584Z",
     "start_time": "2025-01-17T12:26:00.845714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "regressor = GradientBoostingRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = regressor.predict(X_test)\n",
    "\n",
    "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "print(f\"Mean absolute error: {mae_gb:.2f}\")\n",
    "print(f\"Mean squared error: {mse_gb:.2f}\")"
   ],
   "id": "5e112952f235ba20",
   "execution_count": 130,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Neural Network\n",
    "This script performs data preprocessing and trains a neural network to predict user scores for anime titles"
   ],
   "id": "fe8f5a6cc3fbafda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:18.780966Z",
     "start_time": "2025-01-17T12:26:01.764584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the neural network model architecture\n",
    "# Number of input features\n",
    "input_shape = X_train.shape[1]\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(input_shape,)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compiling the model with optimizer, loss function, and evaluation metrics\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mse',  # Mean Squared Error\n",
    "              metrics=['mae'])  # Mean Absolute Error\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluating the model on the test data\n",
    "loss, mae_nn = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test MAE: {mae_nn}')"
   ],
   "id": "b76b2351806640d1",
   "execution_count": 131,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualizing",
   "id": "cfa1f720146dea63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:19.159366Z",
     "start_time": "2025-01-17T12:26:18.780966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Getting the loss and metric values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_mae = history.history['mae']\n",
    "val_mae = history.history['val_mae']\n",
    "\n",
    "# Plotting the training and validation loss over epochs\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the training and validation MAE over epochs\n",
    "plt.plot(train_mae, label='Training MAE')\n",
    "plt.plot(val_mae, label='Validation MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "3e97f6bb9c591d82",
   "execution_count": 132,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:19.392425Z",
     "start_time": "2025-01-17T12:26:19.159366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(y_test)), y_test, label='Actual Values', color='blue')\n",
    "plt.plot(range(len(y_test)), y_pred_svr, label='Support Vector Regression', color='red')\n",
    "# plt.plot(range(len(y_test)), y_pred_dt, label='Decision Tree', color='violet')\n",
    "# plt.plot(range(len(y_test)), y_pred_rf, label='Random Forest', color='orange')\n",
    "# plt.plot(range(len(y_test)), y_pred_knn, label='KNN', color='green')\n",
    "# plt.plot(range(len(y_test)), y_pred_gb, label='Gradient Boosting Regressor', color='yellow')\n",
    "\n",
    "plt.title('Comparison of Predictions from Different Models')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "985bd123be7946cf",
   "execution_count": 133,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:19.581227Z",
     "start_time": "2025-01-17T12:26:19.392425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "models = ['SVR', 'Decision Tree', 'Random Forest', 'KNN', 'Gradient Boosting', 'Neural Network']\n",
    "mae_values = [mae_svr, mae_dt, mae_rf, mae_knn, mae_gb, mae_nn]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, mae_values, color=['#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#9B59B6', 'blue'])\n",
    "\n",
    "plt.title('MAE Comparison for Different Models')\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "plt.xlabel('Models')\n",
    "plt.show()"
   ],
   "id": "6eeefe2be0309234",
   "execution_count": 134,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:33:32.929096Z",
     "start_time": "2025-01-17T12:33:32.600369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# error_ for each model\n",
    "errors_svr = y_test - y_pred_svr\n",
    "errors_dt = y_test - y_pred_dt\n",
    "errors_rf = y_test - y_pred_rf\n",
    "errors_knn = y_test - y_pred_knn\n",
    "errors_gb = y_test - y_pred_gb\n",
    "\n",
    "\n",
    "# plt.hist(errors_svr, bins=20, alpha=0.5, label='SVR', color='red')\n",
    "# plt.hist(errors_dt, bins=20, alpha=0.5, label='Decision Tree', color='blue')\n",
    "plt.hist(errors_rf, bins=20, alpha=0.5, label='Random Forest', color='orange')\n",
    "# plt.hist(errors_knn, bins=20, alpha=0.5, label='KNN', color='green')\n",
    "plt.hist(errors_gb, bins=20, alpha=0.5, label='Gradient Boosting', color='brown')\n",
    "\n",
    "plt.title('Error Distribution for Different Models')\n",
    "plt.xlabel('Error (y_real - y_pred)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "7aec3b8f7c61809",
   "execution_count": 137,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:33:45.349446Z",
     "start_time": "2025-01-17T12:33:45.071832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "predictions = {\n",
    "    'SVR': y_pred_svr,\n",
    "    'Decision Tree': y_pred_dt,\n",
    "    'Random Forest': y_pred_rf,\n",
    "    'KNN': y_pred_knn,\n",
    "    'Gradient Boosting': y_pred_gb\n",
    "}\n",
    "\n",
    "plt.boxplot(predictions.values(), labels=predictions.keys(), patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightblue', color='blue'),\n",
    "            medianprops=dict(color='red'))\n",
    "\n",
    "plt.title('Prediction Distribution for Different Models')\n",
    "plt.ylabel('Predictions')\n",
    "plt.xlabel('Models')\n",
    "plt.show()"
   ],
   "id": "aa1dbb5c7501c3a9",
   "execution_count": 138,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:20.432292Z",
     "start_time": "2025-01-17T12:26:20.411476Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a60a9fdac62aefc0",
   "execution_count": 136,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
